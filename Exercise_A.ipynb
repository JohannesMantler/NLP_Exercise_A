{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Exercise A.1: Preprocessing\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "file = \"Taylor_Swift_Quotes.txt\"\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()                                                 #Lowercase\n",
    "    text = re.sub(r'\\d+', \" \", text)                                    #Remove digits\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))    #Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()                            #Remove überflüssige Leerzeichen\n",
    "    return text\n",
    "\n",
    "with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "cleaned_text = clean(text)\n",
    "\n",
    "print(cleaned_text)\n",
    "\n",
    "#store cleaned text in new file\n",
    "# with open('cleaned_text.txt', 'w', encoding=\"utf-8\") as file:\n",
    "#    file.write(cleaned_text)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Exercise A.2: Tokenization\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenized_text = nltk.word_tokenize(cleaned_text)\n",
    "print(tokenized_text)\n",
    "\n",
    "#store tokenized text in new file\n",
    "# with open('tokenized_text.txt', 'w', encoding=\"utf-8\") as file:\n",
    "#    file.write(tokenized_text)\n"
   ],
   "id": "9aaf15804ce8c215",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "66fcd21bf137d31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Exercise A.2: StopWords\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS             #removes 'is'. 'the' etc\n",
    "\n",
    "print(f\"Number of Stopwords: {len(STOP_WORDS)}\")\n",
    "print(sorted(STOP_WORDS), \"\\n\")\n",
    "\n",
    "filtered_text = [word for word in tokenized_text if word.lower() not in STOP_WORDS]\n",
    "\n",
    "print(filtered_text)\n",
    "\n",
    "#store filtered text in new file\n",
    "# with open('filtered_text.txt', 'w', encoding=\"utf-8\") as file:\n",
    "#    file.write(filtered_text)"
   ],
   "id": "d0123b0679d20bf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Exercise A.3: Stemming and lemmatization\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')        #db for lemmatizing\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#Ohne das würde alles als Nomen behandelt -> PartOfSpeech Tagger wird sichergestellt\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
    "except LookupError:\n",
    "    try:\n",
    "        nltk.download('averaged_perceptron_tagger_eng')\n",
    "    except:\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#übersetzt die Wortarten (POS-Tags), die der NLTK-Tagger ausgibt, in ein Format, das der WordNet-Lemmatizer versteht.\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
    "\n",
    "pos_tags = nltk.pos_tag(filtered_text)  #bestimmt Wortart\n",
    "lemmatized_text = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "\n",
    "\n",
    "print(stemmed_text, \"\\n\")\n",
    "print(lemmatized_text)\n"
   ],
   "id": "4845b4b516864e82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Exercise A.4: Vocabulary\n",
    "\n",
    "dictionary = lemmatized_text            #welchen Text sollte cih verwenden?\n",
    "\n",
    "vocab_set = set(dictionary)\n",
    "\n",
    "vocab_dict = {word: i for i, word in enumerate(sorted(vocab_set), start=1)} #sorted()=alphabetisch\n",
    "\n",
    "print(f\"Vocab size: {len(vocab_set)}\")\n",
    "for w in list(vocab_dict):\n",
    "    print(w, '→', vocab_dict[w])\n"
   ],
   "id": "a61b3f6c107cb4e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T12:25:43.715825Z",
     "start_time": "2025-10-16T12:25:43.709594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "#np.set_printoptions(threshold=np.inf)  # disables truncation\n",
    "\n",
    "oneHotEncoder = np.zeros((len(lemmatized_text), len(vocab_dict)), dtype=int)\n",
    "\n",
    "for i, word in enumerate(lemmatized_text):\n",
    "    if word in vocab_dict:\n",
    "        index = vocab_dict[word] - 1\n",
    "        oneHotEncoder[i, index] = 1\n",
    "\n",
    "print(oneHotEncoder[:10]) #[:10] zeigt die ersten 10 Wörter an"
   ],
   "id": "bbb3fab73156ccab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
